---
title: "Human Activity Recognition"
output: html_document
---

# Summary
Incorrect weightlifting technique with heavy weights can lead to serious injury.  For this reason, an approach that predicts poor technique before and injury occurs, and alerts the lifter to this, is valuable.  This dataset tracks activity data from trackers such as the Nike Fuelband as it pertains to correct execution of weight lifting maneuvers; specifically it links four different kinds of incorrect technique to the motions.  Using a random forest technique, we report a misclassification rate of under 5% and discover the correct answers to the test set provided.

# Setup
The runtime of some of these algorithms is substantial, so we configure R to compile with four threads (corresponding to my two cores that support hyperthreading).  We also set a seed for reproducibility of random values.
```{r}
library(caret)
library(doMC)
registerDoMC(cores = 4)
set.seed(42)
```

Load files, making sure to set empty strings to NA (there are a lot of them).
```{r}
pmlTrainRaw<-read.csv("pml-training.csv",na.strings=c("NA",""))
pmlTest<-read.csv("pml-testing.csv",na.strings=c("NA",""))
```

First seven rows have metadata for the rows, which is unlikely to help our model.  Additionally, columns either have a large number of NAs or zero, so stick to the complete ones.  This gets the dataset from 160 variables to 53, which is a much more tractable problem.
```{r}
pmlTrainRaw<-pmlTrainRaw[,8:160]
pmlTest<-pmlTest[,8:160]

nona<-apply(is.na(pmlTrainRaw),2,sum)==0
pmlTrainRaw<-pmlTrainRaw[,nona]
pmlTest<-pmlTest[,nona]
```

Separate out a validation set so that we don't pollute the test set.  To determine the final answers, we used a 75/25 training/validation set split, however to keep the runtime short when creating markdown we returned it to 10/90.
```{r}
inTrain<-createDataPartition(pmlTrainRaw$classe,p=0.1,list=FALSE)
pmlTrain<-pmlTrainRaw[inTrain,]
pmlValidate<-pmlTrainRaw[-inTrain,]
```

First let's try a decision tree.  This is insufficient at only 52%, but makes a good baseline.
```{r}
modFitDt<-train(classe ~ .,data=pmlTrain,method="rpart")

predValidate<-predict(modFitDt,newdata=pmlValidate)
confusionMatrix(predValidate,pmlValidate$classe)$overall[1]
```

Next we can try LDA, which is much better, at 69%.
```{r}
modFitDt<-train(classe ~ .,data=pmlTrain,method="lda")

predValidate<-predict(modFitDt,newdata=pmlValidate)
confusionMatrix(predValidate,pmlValidate$classe)$overall[1]
```

Random forest is the most reliable at 95% on the validation set.
```{r}
modFitRf<-train(classe ~ .,data=pmlTrain,method="rf")

predValidate<-predict(modFitRf,newdata=pmlValidate)
confusionMatrix(predValidate,pmlValidate$classe)$overall[1]
```

# Final Results
As random forest was the best approach, we will use that to predict the final output.
```{r}
predTest<-predict(modFitRf,newdata=pmlTest)
predTest
```
This is not quite the correct result, but running the random forest with a larger training set did produce the correct answer.